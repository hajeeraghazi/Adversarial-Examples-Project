# Adversarial-Examples-Project

This project demonstrates how adversarial inputs can fool deep learning models, and how adversarial training and other defense methods improve robustness. It includes scripts for training models, generating adversarial examples, evaluating robustness, and visualizing results.

---

## ğŸ“ Project Structure

```
root/
â”‚
â”œâ”€â”€ training.py               # Script to train standard / robust model
â”œâ”€â”€ adversarial_backend.py    # Attack generation, evaluation, robustness logic
â”‚
â”œâ”€â”€ standard_model.pth        # Pretrained standard model (optional)
â”œâ”€â”€ robust_model.pth          # Pretrained robust model (optional)
â”œâ”€â”€ ensemble_model_0.pth      # Pretrained ensemble model (optional)
â”œâ”€â”€ ensemble_model_1.pth
â”œâ”€â”€ ensemble_model_2.pth
â”‚
â”œâ”€â”€ standard_robustness.png   # Robustness comparison plots
â”œâ”€â”€ robust_robustness.png
â”œâ”€â”€ ensemble_robustness.png
â”‚
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md
```

You may additionally create:

```
data/               # Datasets (created manually)
models/             # Saved models
logs/               # Training & evaluation logs
.env.local          # Environment configuration
```

---

## ğŸ”§ Installation

### 1. Clone the repository

```bash
git clone https://github.com/hajeeraghazi/Adversarial-Examples-Project.git
cd Adversarial-Examples-Project
```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate       # Linux/macOS
# or:
venv\Scripts\activate          # Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ” Create `.env.local` File

Create a file named `.env.local` in the **project root** with the following content:

```
DATA_DIR=./data
MODEL_DIR=./models
LOG_DIR=./logs
DEVICE=cuda        # or cpu
SEED=42
```

Explanation:

| Variable    | Purpose                       |
| ----------- | ----------------------------- |
| `DATA_DIR`  | Dataset download or load path |
| `MODEL_DIR` | Saves trained models          |
| `LOG_DIR`   | Stores logs / metrics         |
| `DEVICE`    | `cuda` or `cpu`               |
| `SEED`      | Reproducibility               |

---

## ğŸ“ Create `data/` Folder

Manually create the dataset folder:

```bash
mkdir data
```

If using MNIST / CIFAR-10, PyTorch/TensorFlow will download automatically into:

```
data/mnist/
data/cifar10/
```

Or you can place custom datasets inside `data/`.

---

## ğŸš€ Running the Project

### â–¶ï¸ Train a Standard Model

```bash
python training.py --mode standard
```

### ğŸ”’ Train a Robust (Adversarially-Trained) Model

```bash
python training.py --mode robust
```

### âš ï¸ Generate Adversarial Examples (FGSM / PGD)

```bash
python adversarial_backend.py --attack fgsm --eps 0.03
python adversarial_backend.py --attack pgd --eps 0.03 --steps 40
```

### ğŸ“Š Evaluate Clean vs Adversarial Robustness

Outputs are saved as PNG plots:

* `standard_robustness.png`
* `robust_robustness.png`
* `ensemble_robustness.png`

---

## ğŸ§ª Features

* Generate adversarial examples (FGSM, PGD).
* Train standard and robust (defended) models.
* Evaluate robustness under different attacks.
* Compare multiple model types (standard, robust, ensemble).
* Visual robustness plots included.

---

## ğŸš€ Future Improvements

* Add more attack methods (CW, DeepFool, AutoAttack)
* Add GUI/dashboard for visualization
* Add more datasets (CIFAR-100, TinyImageNet)
* Logging with TensorBoard


