# Adversarial-Examples-Project

This project explores adversarial attacks and defenses on deep learning image classifiers.  
It allows training of standard and robust models, generating adversarial examples, and comparing robustness/performance under attack.

---

## ğŸ“‚ Repository Structure

```

root/
â”‚
â”œâ”€â”€ training.py               # Script to train standard / robust model
â”œâ”€â”€ adversarial_backend.py    # Attack generation, evaluation, robustness logic
â”‚
â”œâ”€â”€ standard_model.pth        # (optional) Pretrained standard model
â”œâ”€â”€ robust_model.pth          # (optional) Pretrained robust model
â”œâ”€â”€ ensemble_model_0.pth      # (optional) Part of ensemble model
â”œâ”€â”€ ensemble_model_1.pth
â”œâ”€â”€ ensemble_model_2.pth
â”‚
â”œâ”€â”€ standard_robustness.png   # Robustness result plots for standard model
â”œâ”€â”€ robust_robustness.png     # Robustness result plots for robust model
â”œâ”€â”€ ensemble_robustness.png   # Robustness result plots for ensemble model
â”‚
â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚
â”œâ”€â”€ frontend files            # (TypeScript, CSS) â€” possibly for UI / dashboard
â”‚   â”œâ”€â”€ Dashboard.tsx
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ globals.css
â””â”€â”€ README.md                 # <- This file

````

> âš ï¸ Note: There is currently **no `data/` folder**, and **no `.env.local` file** in the repository.

---

## âœ… Prerequisites

- Python 3.x  
- (Optional but recommended) A virtual environment  
- Required packages as per `requirements.txt`

---

## ğŸ”§ Setup & Installation

```bash
git clone https://github.com/hajeeraghazi/Adversarial-Examples-Project.git
cd Adversarial-Examples-Project

# (Recommended) Create a virtual environment:
python -m venv venv
source venv/bin/activate     # Linux / macOS
# or `venv\Scripts\activate` on Windows

pip install -r requirements.txt
````

---

## ğŸ” Configuration (Optional but Recommended)

Create a file named `.env.local` at the project root (not included in repo) with configuration variables.
Example:

```
DATA_DIR=./data
MODEL_DIR=./models
LOG_DIR=./logs
DEVICE=cuda   # or cpu
SEED=42
```

* `DATA_DIR` â†’ where dataset (e.g. MNIST, CIFAR-10) will be downloaded or stored
* `MODEL_DIR` â†’ where trained model checkpoints will be saved/read
* `LOG_DIR` â†’ for logs, metrics, or other outputs
* `DEVICE` â†’ `cuda` or `cpu`, depending on GPU availability
* `SEED` â†’ for reproducible results

If you do not use `.env.local`, ensure that defaults in your code paths correspond to actual folders or modify the code accordingly.

---

## ğŸ“ Data Folder (Manually Create If Needed)

Since there is currently **no `data/` folder** in the repo, if your code assumes dataset files locally, you should create:

```
data/
  â”œâ”€â”€ mnist/      # or whichever dataset you use
  â””â”€â”€ cifar10/
```

Alternatively, if your scripts are designed to auto-download datasets, ensure internet connection is available when running for the first time.

Example to create folder manually:

```bash
mkdir -p data/mnist data/cifar10
```

---

## ğŸš€ Usage / Workflow

### 1. Train a Model (Standard or Robust)

```bash
python training.py --mode standard   # train a normal model
python training.py --mode robust     # train an adversarially-trained model
```

Replace arguments (dataset, epochs, etc.) according to your scriptâ€™s parameters.

### 2. Generate Adversarial Examples & Evaluate Robustness

```bash
python adversarial_backend.py --attack fgsm   --eps 0.03 --dataset MNIST
python adversarial_backend.py --attack pgd    --eps 0.03 --steps 40 --dataset CIFAR10
```

This should create adversarial examples, run inference under attack with your model(s), and output robustness metrics/plots (like `*_robustness.png`).

### 3. (Optional) Explore Frontend / Dashboard

There appears to be a UI component (TypeScript + CSS) â€” if you intend to enable a dashboard:

* Ensure you have Node.js / npm installed
* Add appropriate config (e.g. `package.json`)
* Install dependencies and run the frontend (instructions need to be added)

---

## ğŸ“Š What This Project Demonstrates

* Training of clean (standard) and robust (defended) models
* Generation of adversarial examples using common attacks (e.g. FGSM, PGD)
* Evaluation and comparison of model robustness under adversarial attacks
* (Optional) Visualization or UI for comparing results

---

## ğŸ›  Suggestions / Next Steps (To Make Project More Complete)

* Add a `.env.local` or configuration management to define paths
* Add (or enable) a `data/` folder or dataset download logic
* Add argument-parsing and README instructions for all command-line options
* If using the frontend, add `package.json`, build scripts, and instructions for launching the UI
* Add more documentation about what each script does, expected inputs/outputs

